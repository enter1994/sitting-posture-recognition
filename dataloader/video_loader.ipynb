{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f225dfa-4ee6-4be5-b402-ed1e00af0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f2eb09-e994-4d20-8582-893abcec6004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    r\"\"\"A Dataset for a folder of videos. Expects the directory structure to be\n",
    "    directory->[train/val/test]->[class labels]->[videos]. Initializes with a list\n",
    "    of all file names, along with an array of labels, with label being automatically\n",
    "    inferred from the respective folder names.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of dataset. Defaults to 'ucf101'.\n",
    "            split (str): Determines which folder of the directory the dataset will read from. Defaults to 'train'.\n",
    "            clip_len (int): Determines how many frames are there in each clip. Defaults to 16.\n",
    "            preprocess (bool): Determines whether to preprocess dataset. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset='sitting', split='train', clip_len=30, preprocess=False):\n",
    "        # original video data path, preprocessed video data path\n",
    "        self.root_dir, self.output_dir = '/data/moon/datasets/sitting-posture-recognition/dataset', '/data/moon/datasets/sitting-posture-recognition/dataset'\n",
    "        folder = os.path.join('/data/moon/datasets/sitting-posture-recognition/dataset', split)\n",
    "        self.clip_len = clip_len\n",
    "        self.split = split\n",
    "        \n",
    "        # The following three parameters are chosen as described in the paper section 4.1\n",
    "        self.resize_height = 112\n",
    "        self.resize_width = 112\n",
    "        self.crop_size = 112\n",
    "\n",
    "        # Obtain all the filenames of files inside all the class folders\n",
    "        # Going through each class folder one at a time\n",
    "        self.fnames, labels = [], []\n",
    "        for subject in sorted(os.listdir(folder)):\n",
    "            if os.path.isdir(os.path.join(folder, subject)):\n",
    "                for action in os.listdir(os.path.join(folder, subject)):\n",
    "                    label = action\n",
    "                    if os.path.isdir(os.path.join(folder, subject, action)):\n",
    "                        for num in os.listdir(os.path.join(folder, subject, action)):\n",
    "                            if os.path.isdir(os.path.join(folder, subject, action, num)):\n",
    "                                self.fnames.append(os.path.join(folder, subject, action, num)) # /data/moon/datasets/sitting-posture-recognition/dataset/train/마이쭈/A1\n",
    "                                labels.append(label)\n",
    "\n",
    "        assert len(labels) == len(self.fnames)\n",
    "        print('Number of {} videos: {:d}'.format(split, len(self.fnames)))\n",
    "\n",
    "        # Prepare a mapping between the label names (strings) and indices (ints)\n",
    "        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
    "        \n",
    "        # Convert the list of label names into an array of label indices\n",
    "        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Loading and preprocessing.\n",
    "        buffer = self.load_frames(self.fnames[index])\n",
    "        labels = np.array(self.label_array[index])\n",
    "        \n",
    "        buffer = self.normalize(buffer)\n",
    "\n",
    "        buffer = self.to_tensor(buffer)\n",
    "\n",
    "        return torch.from_numpy(buffer), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "    def randomflip(self, buffer):\n",
    "        \"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            for i, frame in enumerate(buffer):\n",
    "                frame = cv2.flip(buffer[i], flipCode=1)\n",
    "                buffer[i] = cv2.flip(frame, flipCode=1)\n",
    "\n",
    "        return buffer\n",
    "\n",
    "\n",
    "    def normalize(self, buffer):\n",
    "        for i, frame in enumerate(buffer):\n",
    "            frame -= np.array([[[90.0, 98.0, 102.0]]])\n",
    "            buffer[i] = frame\n",
    "\n",
    "        return buffer\n",
    "\n",
    "    def to_tensor(self, buffer):\n",
    "        return buffer.transpose((3, 0, 1, 2))\n",
    "\n",
    "    def load_frames(self, file_dir):\n",
    "        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n",
    "        frame_count = len(frames)\n",
    "\n",
    "        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n",
    "        for i, frame_name in enumerate(frames):\n",
    "            frame = np.array(cv2.imread(frame_name)).astype(np.float64)\n",
    "            frame = cv2.resize(frame, (self.resize_width, self.resize_height))\n",
    "\n",
    "            buffer[i] = frame\n",
    "\n",
    "        return buffer\n",
    "\n",
    "    def crop(self, buffer, clip_len, crop_size):\n",
    "        # randomly select time index for temporal jittering\n",
    "        time_index = np.random.randint(buffer.shape[0] - clip_len)\n",
    "\n",
    "        # Randomly select start indices in order to crop the video\n",
    "        height_index = np.random.randint(buffer.shape[1] - crop_size)\n",
    "        width_index = np.random.randint(buffer.shape[2] - crop_size)\n",
    "\n",
    "        # Crop and jitter the video using indexing. The spatial crop is performed on\n",
    "        # the entire array, so each frame is cropped in the same location. The temporal\n",
    "        # jitter takes place via the selection of consecutive frames\n",
    "        buffer = buffer[time_index:time_index + clip_len,\n",
    "                 height_index:height_index + crop_size,\n",
    "                 width_index:width_index + crop_size, :]\n",
    "\n",
    "        return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4ac7c0-c441-4f92-a465-06950c34eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9f6e4c-3765-407e-9766-f796909bce94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     train_data = VideoDataset(dataset='sitting', split='train', clip_len=30, preprocess=False)\n",
    "#     train_loader = DataLoader(train_data, batch_size=6, shuffle=True)\n",
    "\n",
    "#     next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cec0db-b69f-4b7b-9488-8d756b1e94e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
